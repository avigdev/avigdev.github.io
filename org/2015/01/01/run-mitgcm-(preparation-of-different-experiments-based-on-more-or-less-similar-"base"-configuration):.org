* run mitgcm (preparation of different experiments based on more or less similar "base" configuration): 
  :PROPERTIES:
  :date:     2015/01/01 07:14:50
  :updated:  2015/01/01 07:15:16
  :END:

prepare (if you haven't already done so ) an optfile based on the examples under ~/MITgcm/tools/build_options . put it on ~/MITgcm/
my optfile is named opt_ocean (so change this name to your name in the following)

create a directory (anywhere on the cluster) with the following subdirs:
mkdir input
mkdir code
mkdir bin

in input put your data files. the (almost) minimal set is :
data, (data.mnc,data.{other packages}),data.pkg,eedata,eedata.mth, (gendata.m , if you want to generate binary files)

in code put your header files. the minimal set is:
CPP_OPTIONS.h  packages.conf  SIZE.h  SIZE.h_mpi

if your current configuration is named config01, prepare inside input the file data_config01, and inside code the file SIZE.h_config01
also run the following, from the base dir : 
mkdir input_config01 code_config01 build_config01 run_config01


in bin put your binary files, which are the *pickups* and the results of the following command (run it from the base directory) :
grep File input/data_config01 | awk '{print $3 }' | sed "s/'//g" | sed s/,//g | paste -sd" "

run the following under the base directory:

cd input_config01
cp ../input/* ./
ln -s ../bin/{relevant_bin_file} ./
mv data_config01 data
cd ..
cd code_config01
cp ../code/* ./
mv SIZE.h_config01 SIZE.h

the bin files are not copied since they are heavier. 
if you change more than just SIZE.h and data (e.g. if you change data.obcs, etc.) - apply the same principles for the other files...

to build the model program, run from the base dir:
cd build_config01
~/MITgcm/tools/genmake2 -rd=$HOME/MITgcm -mods=../code_config01 -of /home/avigoz/MITgcm/opt_ocean -mpi 2> stam1.txt
make depend 2> stam2.txt
make -j 2> stam3.txt

if anything goes wrong - it is indicated in stam{X}.txt

to run on the cluster, I use the script slurm_run.pl . go to the base dir and do :
cd run_config01
ln -s ../input_config01/* ./
ln -s ../build_config01/mitgcmuv ./mitgcmuv_config01
slurm_run.pl {number_of_mpi_processes} ./mitgcmuv_config01

the main reason for the above setup - is that it can easily be automated and done many times for different configurations, in very little effort. it also keeps everything organized by the different experiments.... 
you can see your job listed (or not- if anything goes wrong), if you type the following command : 
squeuecpus

to cancel a run use: 
scancel {number_of_job}
