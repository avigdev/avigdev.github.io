* simplify slurm and qsub  
  :PROPERTIES:
  :date:     2014/12/28 18:48:02
  :categories: slurm,qsub,orgmode
  :updated:  2015/01/12 20:02:52
  :END:

[[http://slurm.schedmd.com][slurm]] and qsub (link anyone?) are beautiful cluster schedulers. If you work on a cluster, you probably use one. I use both, as well as some old computers which don't have schedulers. I manage my runs from an [[http://orgmode.org][orgmode]] "notebook", with a table that tells my scripts which resource uses which scheduler. 

The usual way to use slurm and qsub is by submitting a little shell script which tells all the nodes how to divide their tasks, what are the important environment variables, which command are we running, etc. If you work on clusters you probably have a zillion copies of these little scripts.  

/FIRST,/ most of the information is identical, so why not create a template at the home directory ? Instead of the absolute path of the current run, insert %s, instead of the number of mpi threads insert %d ... you get the idea. I call my template .slurm_cmds . 

Now, we need to automatically create templates by replacing all those %x by our real information, and submit to the queue:

#+NAME: slurm_run
#+BEGIN_SRC perl -n :exports code :eval never
#!/usr/bin/perl -w
# purpose : insert a job to the slurm queue
# syntax : slurm_run.pl number_of_processes cmd
# number_of_processes= the number of cores that are expected to be used by
# the job. this is not verified - so consistency with the compilation under
# MPI is just assumed and is the responsibility of the user. 
# cmd = the executable (usually binary) you wish to include in the queue 
# the file .slurm_cmds is expected to be found on the $HOME directory.
# this file is a template batch file with all the needed exports and a srun
# call. slurm_run.pl just reads the template, replaces the necessary info to
# the right places, and sends the new formed batch file to the queue.
#
# depends on : (1) the perl Env and Cwd libraries ,
# (2) the $HOME/.slurm_cmds template
#
# Copyright 2012 Avi Gozolchiani (http://tiny.cc/avigoz)
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# $Log$
use Env;
use Cwd;
$currWorkDir = &Cwd::cwd();
# parse cmd line
$n_proc=shift //die "syntax error : slurm_run number_of_processes cmd\n";
$cmd=shift //die "syntax error : slurm_run number_of_processes cmd\n";
# define file names (both source and target)
$slurm_template="$HOME/.slurm_cmds";
$batch_name="run-mit.batch_$1";
# open the files
open SLURMTEMP, $slurm_template or die "couldn't find the template file\n";
open BATCH,">$batch_name" or die "couldn't write a temporary batch file\n";
# copy each line from the source template to the target, with
# the necessary changes
while(<SLURMTEMP>){
    last if length($_)==0;
    if(/cd/){
        printf BATCH $_,$currWorkDir;
    }elsif(/srun/){ # if(/cd/){
        printf BATCH $_ , $n_proc, $cmd;
    }elsif(/SBATCH/){ # if(/cd/){ ... }elsif(/srun/){
        printf BATCH $_, $n_proc;
    }else{   # if(/cd/){... }elsif(/srun/){...}elsif(/SBATCH/){
        print BATCH $_;
    }        # if(/cd/){... }elsif(/srun/){...}elsif(/SBATCH/){..}else{
}                               # while(<SLURMTEMP>){
close BATCH;
# send to queue
print `sbatch -x n03 ./$batch_name`;
#+END_SRC

The last line submits my fresh batch file to the slurm queue. I can monitor it's processing via :
#+BEGIN_SRC sh -n :exports code :eval never
squeue  -o '%.7i %.9P %.50j %.8u %.2t %.10M %.5D %.6C %R'
#+END_SRC


the "%.50j" is important, since we want to know the full job names.

The "-x n03" part in slurm_run.pl was added since our system admin asked me to not use node 03. Is there a better way to consistently do it?
